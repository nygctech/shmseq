{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing raw counts to anndata file\n",
    "\n",
    "Input files: <br>\n",
    "patches: $*$.jpg<br>\n",
    "HE images: $*$HE.jpg<br>\n",
    "Annotation files: $*$annotations.txt<br>\n",
    "Metadata files: ASF_Metadata_final.csv / wtgf_metadata.xlsx<br>\n",
    "Probability files: $*$Probabilities.tiff stored as sample_probabilities.tar.gz<br>\n",
    "Aligned counts files: $*$stdata_under_tissue_IDs.txt<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob\n",
    "from PIL import Image\n",
    "from skimage import io\n",
    "from skimage.color import rgb2gray\n",
    "from sklearn import preprocessing\n",
    "import skimage\n",
    "from skimage import filters\n",
    "import math\n",
    "import scanpy as sc\n",
    "import scipy.ndimage as ndi\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from skimage import data, img_as_float, exposure, util, measure, feature, morphology, segmentation, color\n",
    "from skimage.morphology import reconstruction, closing, square\n",
    "from skimage.filters import sobel\n",
    "from skimage.segmentation import clear_border, watershed\n",
    "from skimage.measure import label, regionprops\n",
    "from skimage.util import invert\n",
    "from skimage.util import img_as_ubyte\n",
    "from skimage.measure import label, regionprops, regionprops_table\n",
    "from skimage import draw\n",
    "import anndata as adata\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "import gzip\n",
    "import pickle\n",
    "from scipy.interpolate import LinearNDInterpolator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_patch_translation(x, y, image_rescaled):\n",
    "    \"\"\"\n",
    "    x and y mimick original patch values: 31_32\n",
    "    \"\"\" \n",
    "    # get size of one patch\n",
    "    testy = image_rescaled.shape[1]/32\n",
    "    testx = image_rescaled.shape[0]/34\n",
    "\n",
    "    x_indices = (x-1)*testy\n",
    "    y_indices = (y-1)*testx\n",
    "    \n",
    "    # Make a square area of each spot\n",
    "    # needs to be the same code as in cropping\n",
    "    xmin = float(x_indices)-image_rescaled.shape[0]/(image_rescaled.shape[0]/100)\n",
    "    ymin = float(y_indices)-image_rescaled.shape[1]/(image_rescaled.shape[0]/120)\n",
    "    xmax = float(x_indices)+image_rescaled.shape[0]/(image_rescaled.shape[0]/100)\n",
    "    ymax = float(y_indices)+image_rescaled.shape[1]/(image_rescaled.shape[0]/120)\n",
    " \n",
    "    return x_indices, y_indices, xmin, xmax, ymin, ymax\n",
    "\n",
    "def mask_disk(maskimg):\n",
    "    \"\"\"\n",
    "    Makes disk inside a voxel to mimick ST spots.\n",
    "\n",
    "    Returns a masked image minus the area outside the disk.\n",
    "    \"\"\"\n",
    "    \n",
    "    # mask a \"ST spot\" from the image\n",
    "    rr1, cc1 = draw.ellipse(110, 100, r_radius=100, c_radius=100, shape=maskimg.shape)\n",
    "    mask1 = np.zeros_like(maskimg, dtype=np.bool)\n",
    "    mask1[rr1, cc1] = True\n",
    "    maskimg *= mask1\n",
    "    \n",
    "    return maskimg\n",
    "\n",
    "def remove_background_HE(maskimg, heimg, connectivity, min_size, stripped_name):\n",
    "    \"\"\"\n",
    "    Removes background and gets estimate of cells close to each other \n",
    "    based on disk cut offs. There cells are \"dividing\" cells and will\n",
    "    not be split into two.\n",
    "    \n",
    "    Return a binary mask, labels of individual cells in the binary mask\n",
    "    and a pandas df with centroids of all nuclei in one ST spot.\n",
    "    \"\"\"\n",
    "       \n",
    "    # get threshold and filter (gets cells close together)\n",
    "    thresholds = filters.threshold_multiotsu(maskimg, classes=3)\n",
    "    cells = maskimg > 0.02\n",
    "\n",
    "    # gets neighboring distances between cells\n",
    "    distance = ndi.distance_transform_edt(cells)\n",
    "\n",
    "    # sets max distance\n",
    "    local_maxi = feature.peak_local_max(distance, indices=False,\n",
    "                                    min_distance=connectivity)\n",
    "    #labels objects at the max distance\n",
    "    markers = measure.label(local_maxi)\n",
    "\n",
    "    # segmetns cells\n",
    "    segmented_cells = segmentation.watershed(-distance, markers, mask=cells)\n",
    "\n",
    "    #remove small objects\n",
    "    cleaned = morphology.remove_small_objects(segmented_cells, min_size=min_size) # 5000\n",
    "    \n",
    "    # gets centroids of each nuclear segment\n",
    "    label_img = label(cleaned)\n",
    "    regions = regionprops(label_img)\n",
    "    \n",
    "    #makes all the plots\n",
    "    fig, ax = plt.subplots(ncols=5, figsize=(20, 10))\n",
    "    ax[0].imshow(cells, cmap='gray')\n",
    "    ax[0].set_title('Overlapping nuclei')\n",
    "    ax[0].axis('off')\n",
    "    ax[1].imshow(color.label2rgb(segmented_cells, bg_label=0))\n",
    "    ax[1].set_title('Segmented nuclei')\n",
    "    ax[1].axis('off')\n",
    "    ax[2].imshow(color.label2rgb(cleaned, bg_label=0))\n",
    "    ax[2].set_title('Cleaned image')\n",
    "    ax[2].axis('off')\n",
    "    ax[3].imshow(maskimg)\n",
    "    ax[3].set_title('Original image')\n",
    "    ax[3].axis('off')\n",
    "    ax[4].imshow(heimg)\n",
    "    for props in regions:\n",
    "        y0, x0 = props.centroid\n",
    "        ax[4].plot(round(x0), round(y0), '.g', markersize=5)\n",
    "    ax[4].set_title('Segment centroids')\n",
    "    ax[4].axis('off')\n",
    "\n",
    "    plt.show()\n",
    "       \n",
    "    # variable with all the different segments\n",
    "    labels = color.label2rgb(cleaned, bg_label=0)\n",
    "    \n",
    "    # df with the centroids of each nucleus\n",
    "    rows = []\n",
    "    for props in regions:\n",
    "        y0, x0 = props.centroid\n",
    "        rows.append([x0, y0])\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=[\"x\", \"y\"])\n",
    "    \n",
    "    return cleaned, labels, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get path to training directory per image \n",
    "train_path = '/patches'\n",
    "# path with original HE files\n",
    "he_path = '/images'\n",
    "\n",
    "#list all dirs in the train_path\n",
    "imgs = [os.path.basename(i) for i in glob.glob(os.path.join(train_path, '*'))]\n",
    "\n",
    "# reads in metadata for this sample to put it into a final df \n",
    "metadata = pd.read_excel('/metadata.xlsx',\n",
    "                        usecols=['Genotype', 'Specimen #ID','Filename', 'Sex'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of already processed files\n",
    "h5ad_path = '/raw_data' #output folder\n",
    "data_ready = [str(i).split(\".h5ad\")[0] for i in os.listdir(h5ad_path)]\n",
    "\n",
    "for image_name in imgs:\n",
    "    \n",
    "    # check if this image has already been processed\n",
    "    if (image_name in data_ready):\n",
    "        continue \n",
    "        \n",
    "    if not image_name in metadata['Filename'].tolist():\n",
    "        continue\n",
    "    \n",
    "    # Get names of all patches for one array\n",
    "    name =  glob.glob(os.path.join(train_path, image_name, '*jpg'))\n",
    "    name_list = [str(i).split(\"/\")[-1] for i in name]\n",
    "\n",
    "    # Read in large HE file\n",
    "    large_he_jpg_name = os.path.join(he_path, image_name + '_HE.jpg')\n",
    "    heimg_large = Image.open(large_he_jpg_name)\n",
    "    heimg_large = np.array(heimg_large)\n",
    "\n",
    "    # this df collect all centroids from all patches\n",
    "    centroids_df = pd.DataFrame(columns = [\"patch\", \"centroids\", \"_x\", \"_y\", \"x\", \"y\", \"xy\", \"sample\", \"cell_count\"])\n",
    "    \n",
    "    # load splotch annotations files\n",
    "    splotch_ann_patch = \"/annotations\"\n",
    "    splotch_ann_file = image_name + '_annotations.txt'\n",
    "    splotch_ann = pd.read_csv(os.path.join(splotch_ann_patch, splotch_ann_file), sep = \"\\t\")\n",
    "    splotch_ann['x'] = [str(round(float(i))) for i in splotch_ann['x']]\n",
    "    splotch_ann['y'] = [str(round(float(i))) for i in splotch_ann['y']]\n",
    "    splotch_ann['x_y'] = [str(i)+\"_\"+str(j) for i,j in zip(splotch_ann['x'], splotch_ann['y'])]\n",
    "    splotch_ann['patch'] = [i+\"_\"+j for i,j in zip(splotch_ann['image'], splotch_ann['x_y'])]\n",
    "    print(\"Spots in annotation file...\", len(splotch_ann))\n",
    "\n",
    "    counter = 0\n",
    "    for file in name_list:\n",
    "\n",
    "        # read in probabilities from ilastik (tiff, u16) and matching HE small patch (jpg)\n",
    "        stripped_name = file.split(\".jpg\")[0]\n",
    "        prob_name = os.path.join(train_path, image_name, stripped_name + '_Probabilities.tiff')\n",
    "        he_jpg_name = os.path.join(train_path, image_name, stripped_name + '.jpg')\n",
    "        print(\"Processing patch...\")\n",
    "        print(stripped_name)\n",
    "           \n",
    "        # Load probabilities mask\n",
    "        maskimg = Image.open(prob_name).convert('L')\n",
    "        maskimg = np.array(maskimg)\n",
    "        heimg = Image.open(he_jpg_name)\n",
    "        heimg = np.array(heimg)\n",
    "\n",
    "        # Remove backgorund of mask image\n",
    "        print('Removing background, segmenting and counting...')\n",
    "\n",
    "        # Getting ST-spot like images \n",
    "        maskimg = mask_disk(maskimg)\n",
    "\n",
    "        # in case the disk creates and empty image, skip this patch\n",
    "        if (maskimg.mean() < 0.05):\n",
    "            # print(\"Skipping array ... No cells left after disk masking\")\n",
    "            cell_indicator = 0\n",
    "        else:\n",
    "            cell_indicator = 1\n",
    "            #continue\n",
    "            \n",
    "        # check spot annotation value to adjust thersholding in otsu\n",
    "        if (len(np.intersect1d(['muscle','interna','externa' ,'mucosae','externa and interna'],splotch_ann[splotch_ann['patch'] == stripped_name]['value'].tolist())) > 0):\n",
    "            otsu_size = 50\n",
    "            otsu_connectivity = 15\n",
    "        else: \n",
    "            otsu_size = 50\n",
    "            otsu_connectivity = 7\n",
    "        \n",
    "        # thresholds morphology.watershed\n",
    "        if cell_indicator == 1:\n",
    "            mask_bgrm, labels, centrs = remove_background_HE(maskimg, heimg, otsu_connectivity, otsu_size, stripped_name) # 1, 0\n",
    "\n",
    "            # get x_coors and y_coors from patch name\n",
    "            x_coors = str(round(float(stripped_name.split(\"_\")[2])))\n",
    "            y_coors = str(round(float(stripped_name.split(\"_\")[3])))\n",
    "            x_indices, y_indices, xtransmin, xtransmax, ytransmin, ytransmax = get_patch_translation(float(x_coors), float(y_coors), heimg_large)\n",
    "\n",
    "            # transform centroids from patches to match centroids in the whole large HE image\n",
    "            centrs['x_trans'] = centrs['x'] + xtransmin \n",
    "            centrs['y_trans'] = centrs['y'] + ytransmin \n",
    "\n",
    "            # make a list of centroids from a single patch\n",
    "            centrs_list = pd.Series([[int(round(x)),int(round(y))] for x, y in zip(centrs['x_trans'], centrs['y_trans'])])\n",
    "\n",
    "            # collect centroids from all patches into a pandas df\n",
    "            centroids_df.at[counter, 'centroids'] = centrs_list.tolist()\n",
    "            centroids_df.at[counter, 'patch'] = str(x_coors)+'_'+str(y_coors)\n",
    "            centroids_df.at[counter, '_x'] = '_'+str(x_coors)\n",
    "            centroids_df.at[counter, '_y'] = '_'+str(y_coors)\n",
    "            centroids_df.at[counter, 'x'] = x_indices\n",
    "            centroids_df.at[counter, 'y'] = y_indices\n",
    "            centroids_df.at[counter, 'sample'] = image_name # change when this is finalized\n",
    "            centroids_df.at[counter, 'cell_count'] =  len(centrs_list)\n",
    "        else:\n",
    "            # get x_coors and y_coors from patch name\n",
    "            x_coors = str(round(float(stripped_name.split(\"_\")[2])))\n",
    "            y_coors = str(round(float(stripped_name.split(\"_\")[3])))\n",
    "            x_indices, y_indices, xtransmin, xtransmax, ytransmin, ytransmax = get_patch_translation(float(x_coors), float(y_coors), heimg_large)\n",
    "            # collect centroids from all patches into a pandas df\n",
    "            centroids_df.at[counter, 'centroids'] = []\n",
    "            centroids_df.at[counter, 'patch'] = str(x_coors)+'_'+str(y_coors)\n",
    "            centroids_df.at[counter, '_x'] = '_'+str(x_coors)\n",
    "            centroids_df.at[counter, '_y'] = '_'+str(y_coors)\n",
    "            centroids_df.at[counter, 'x'] = x_indices\n",
    "            centroids_df.at[counter, 'y'] = y_indices\n",
    "            centroids_df.at[counter, 'sample'] = image_name # change when this is finalized\n",
    "            centroids_df.at[counter, 'cell_count'] =  0\n",
    "\n",
    "        counter = counter + 1\n",
    "\n",
    "    # final df\n",
    "    splotch_ann.drop([\"x\",\"y\"], axis = 1, inplace = True)\n",
    "    df_all = pd.merge(left=splotch_ann, right=centroids_df, left_on='x_y', right_on='patch')\n",
    "    df_all = pd.merge(left=df_all, right=metadata, left_on='sample', right_on='Filename')\n",
    "    df_all = df_all[['sample', 'x_y', 'value', 'x', 'y', 'Genotype', 'Specimen #ID', 'cell_count', 'Sex']]\n",
    "    df_all = df_all.rename(columns={\"Specimen #ID\": \"Specimen_ID\"})\n",
    "    df_all = df_all.rename(columns={\"x_y\": \"patch\"})\n",
    "    df_all = df_all.rename(columns={\"value\": \"annotation\"})\n",
    "    \n",
    "    # Lists all avaialable st data files\n",
    "    counts_path = '/aligned_counts' # this is as outputed with spotter /x_y and ensums\n",
    "    counts = pd.read_csv(os.path.join(counts_path, df_all['sample'][0]+'_stdata_under_tissue_IDs.txt'), sep =\"\\t\")\n",
    "    counts_df = pd.DataFrame(counts)\n",
    "    counts_df.columns = [str(round(float(i.split(\"_\")[0])))+\"_\"+str(round(float(i.split(\"_\")[1]))) for i in counts_df.columns]\n",
    "    \n",
    "    #rename gene names\n",
    "    names = pd.read_csv('/Gene_names_mm.txt', sep =\"\\t\", header=None, names = ['gene'])\n",
    "    names.index = [str(i).split('_')[0] for i in names['gene']]\n",
    "    #print(names)\n",
    "    names['gene_names'] = [str(i).split('_')[1] for i in names['gene']]\n",
    "    names = names.drop(columns = 'gene')\n",
    "    \n",
    "    # merges df from pre-processing steps with the correct gene names\n",
    "    # subset to only hold gene names in st data\n",
    "    tmp = names[names.index.isin(counts_df.index)]\n",
    "    counts_df.index = list(tmp['gene_names']) + [i for i in counts_df.index if not 'ENS' in i]\n",
    "    counts_df_t = counts_df.T\n",
    "\n",
    "    # df with st data and metadata in correct order\n",
    "    df_vars = pd.merge(left=df_all, right=counts_df_t, left_on='patch', right_index=True)\n",
    "    df_vars.index = [str(i)+'_'+str(j) for i,j in zip(df_vars['sample'],df_vars['patch'])]\n",
    "    #print(\"Spots after merging counts...\", len(df_vars))\n",
    "\n",
    "    # df with st gene data in correct order\n",
    "    df_genes = df_vars.iloc[:,9:]\n",
    "    df_genes.index = [str(i)+'_'+str(j) for i,j in zip(df_vars['sample'],df_vars['patch'])]\n",
    "\n",
    "    # make anndata object that with go into tangram\n",
    "    space_anndata = adata.AnnData(obs = df_vars.iloc[:,:10].astype({'x': 'int64', 'y': 'int64', 'cell_count': 'int64'}), X = df_genes)\n",
    "\n",
    "    # makes sure all gene names are unique\n",
    "    space_anndata.var_names_make_unique()\n",
    "\n",
    "    # makes sure the obs_names does not start with a number as that isn't supported in anndata\n",
    "    space_anndata.obs_names = [str(i)+'_'+str(j) for i,j in zip(space_anndata.obs['sample'],space_anndata.obs['patch'])]\n",
    "\n",
    "    # collects total gene counts in a new variable used later for filtering\n",
    "    space_anndata.var['n_counts'] =[int(i) for i in space_anndata.X.sum(axis=0)]\n",
    "\n",
    "    # writes h5ad file \n",
    "    space_anndata.write_h5ad(filename = os.path.join(h5ad_path, image_name+'.h5ad'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all data segmented ...\n"
     ]
    }
   ],
   "source": [
    "# checks if any files are missing based on metadata\n",
    "if len(data_ready) == len(metadata.index):\n",
    "    print(\"all data segmented ...\")\n",
    "else: \n",
    "    print(\"These files don't match: \")\n",
    "    print(np.setdiff1d(metadata['Filename'], data_ready))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
